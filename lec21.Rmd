---
title: "STA286 Lecture 21"
author: "Neil Montgomery"
date: "Last edited: `r format(Sys.time(), '%Y-%m-%d %H:%M')`"
output: 
  beamer_presentation:
    keep_tex: TRUE
    incremental: TRUE
    df_print: tibble
    fig_caption: FALSE
classoption: aspectratio=169
header-includes:
- \renewcommand{\le}{\leqslant}
- \renewcommand{\ge}{\geqslant}
- \renewcommand\P[1]{P{\left(#1\right)}}
- \newcommand\F[1]{F_{\tiny{#1}}}
- \newcommand\f[1]{f_{\tiny{#1}}}
- \newcommand\p[1]{p_{\tiny{#1}}}
- \newcommand\M[1]{M_{\tiny{#1}}}
- \newcommand\V[1]{\text{Var}\!\left(#1\right)}
- \newcommand\E[1]{E\!\left(#1\right)}
- \newcommand\N[1]{N_{\tiny{#1}}}
- \newcommand\ol{\overline}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE)
options(tibble.width=70)
```

## a quick note---$n$ "large enough" in the Bernoulli($p$) special case

The probability $p$ is in some sense a "shape" parameter for Binomial distributions, in the sense that the close $p$ is to 0 (or 1), the more right (left) skewed the distribution is.

\pause It has been observed through experience and simulation that:
$$np \ge 5 \qquad \text{ and } \qquad n(1-p)\ge 5$$
is sufficient for a good normal approximation. 

\pause Although this suggestion depends on $p$, which is usually unknown. 

\pause We'll revisit this issue when the time comes to discuss ways to evaluate the empirical accuracy of a normal approximation. 

## more normal-based "sampling distributions"

The main focus will be on $\ol{X}$ and friends, because the most common statistical problem is to make statements about an unknown population mean $\mu$ and $\ol{X}$ will be used as the guess.

\pause It turns out $\ol{X}$ is BFF with the sample variance:
$$S^2 = \frac{\sum\limits_{i=1}^n\left(X_i - \ol{X}\right)^2}{n-1}$$

\pause Why? Because the analysis of the properties of $\ol{X}$ will depend on:
$$\frac{\ol{X} - \mu}{\sigma/\sqrt{n}} \sim \onslide<4->{\text{ (or }\sim^{approx})}\ N(0,1)$$
\pause \pause but typically $\sigma$ is also unknown, and $\sqrt{S^2}$ will be used as \textit{its} guess.

## the distribution of the sample variance $S^2$ - I

First we'll consider the distribution of $Z^2$ when $Z \sim N(0,1)$.

\begin{align*}
\M{Z^2}(t) = \E{e^{tZ^2}} &= \int\limits_{-\infty}^{\infty}e^{tz^2}\frac{1}{\sqrt{2\pi}}e^{-z^2/2}\,dz \\
\onslide<2->{&= \int\limits_{-\infty}^{\infty}\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}z^2(1-2t)}\,dz\\}
\onslide<3->{&= \onslide<5->{\left(\frac{0.5}{0.5-t}\right)^{0.5}}\int\limits_{-\infty}^{\infty}\frac{1}{\onslide<4->{\left(\frac{0.5}{0.5-t}\right)^{0.5}}\sqrt{2\pi}}e^{ 
  -\frac{1}{2}\left(\frac{z-0}{\left(\frac{0.5}{0.5-t}\right)^{0.5}}\right)^2}\,dz}
\end{align*}

\pause\pause\pause\pause\pause The integrand is a $N\left(0,\frac{0.5}{0.5-t}\right)$ density, so the integral equals 1, leaving $\M{Z^2}(t) = \left(\frac{0.5}{0.5-t}\right)^{0.5}$

## the distribution of the sample variance $S^2$ - II

A look at the list of mgfs reveals that $Z^2 \sim \text{Gamma}\left(\alpha=\frac{1}{2},\lambda=\frac{1}{2}\right)$

\pause Synonym: If $X \sim \text{Gamma}\left(\alpha=\frac{\nu}{2},\lambda=\frac{1}{2}\right)$ then we give $X$ a special name: "chi-square" distribution is parameter $\nu$.
$$X \sim \chi^2_\nu$$

\pause The parameter $\nu$ gets a curious name: \textit{degrees of freedom}. 

\pause \textbf{Theorem:} If $Z_1,\ldots,Z_n$ are i.i.d. $N(0,1)$, then $\sum\limits_{i=1}^n Z_i^2 \sim \chi^2_n$.

\pause \textbf{Therefore:} If $X_1,\ldots,X_n$ are i.i.d. $N(\mu, \sigma)$, then:
$$\sum\limits_{i=1}^n \left(\frac{X_i - \mu}{\sigma}\right)^2 \sim \chi^2_n$$

\pause \textbf{Theorem:} If $X$ and $Y$ are independent with $X \sim \chi^2_n$ and $X + Y \sim \chi^2_{n+m}$, then $Y \sim \chi^2_m$.

## the distribution of the sample variance $S^2$ - III

\textbf{Magical truth:} If $X_1,\ldots,X_n$ are i.i.d. $N(\mu, \sigma)$, then $\overline{X}$ and $S^2$ are independent.

\pause \textbf{Finally:} If $X_1,\ldots,X_n$ are i.i.d. $N(\mu, \sigma)$:

\begin{align*}
\sum\limits_{i=1}^n \left(\frac{X_i - \mu}{\sigma}\right)^2 
\onslide<3->{&=\frac{1}{\sigma^2}\sum\limits_{i=1}^n \left(X_i - \ol{X}+\ol{X} - \mu\right)^2\\}
\onslide<4->{&= \frac{1}{\sigma^2}\sum\limits_{i=1}^n \left(X_i - \ol{X}\right)^2+\frac{1}{\sigma^2}\sum\limits_{i=1}^n \left(\ol{X} - \mu\right)^2 + 0\\}
\onslide<5->{&= \frac{n-1}{\sigma^2}S^2 + \left(\frac{\ol{X} - \mu}{\sigma/\sqrt{n}}\right)^2}
\end{align*}

\pause\pause\pause\pause Conclusion: $\frac{n-1}{\sigma^2}S^2 \sim \chi^2_{n-1}$

## the $t$ distributions - I

I mentioned earlier that properties of $\ol{X}$ will depend on:
$$\frac{\ol{X} - \mu}{\sigma/\sqrt{n}} \sim N(0,1)$$
and that $\sigma$ will have to be guessed by $S=\sqrt{S^2}$.

\pause However, when the constant $\sigma$ is replaced with the random variable $S$, the result is no longer $N(0,1)$. 

\pause It turns out it is possible to derive the density of:
$$T = \frac{\ol{X} - \mu}{S/\sqrt{n}}$$

## the $t$ distributions - II

The density $f(t)$ of $T$ is is primarily nice to look at:
$$f(t) = \frac{\Gamma[(\nu+1)/2]}{\Gamma[\nu/2]\sqrt{\pi}\nu}\left(1 + \frac{t^2}{\nu}\right)^{-(\nu + 1)/2}$$
where $\nu$ is called the "degrees of freedom" and in this case is $n-1$. You can think of $n-1$ as having been "inherited" from the denominator of $T$.

\pause Important properties:

* Symmetric and bell shaped.

* As $\nu$ gets big, form of $f(t)$ approaches $e^{-t^2/2}$...

* ...in other words \textit{as the sample size gets large, $T$ starts to look like $Z \sim N(0,1)$}

* no anti-derivative, so a table of $t$ probabilities needed on tests.

## overall pictures of $t_\nu$

```{r}
z <- -400:400/100
plot(z, dnorm(z), xlab="z", ylab="density", type="l")
lines(z, dt(z, 3), col="red")
lines(z, dt(z, 10), col="blue")
lines(z, dt(z, 30), col="violet")
legend(1.5, 0.3, legend = c("N(0,1)", "t_3", "t_10", "t_30"), fill = c("black", "red", "blue", "violet"))
```

## pictures of $t_\nu$ in the "tail"

```{r}
z <- 200:500/100
plot(z, dnorm(z), xlab="z", ylab="density", type="l")
lines(z, dt(z, 3), col="red")
lines(z, dt(z, 10), col="blue")
lines(z, dt(z, 30), col="violet")
lines(z, dt(z, 100), col="darkgreen")
legend(3, 0.05, legend = c("N(0,1)", "t_3", "t_10", "t_30", "t_100"), fill = c("black", "red", "blue", "violet", "darkgreen"))
```

## the $F$ distributions

We might need the following technical result as well.

\pause If $U\sim \chi^2_m$ and $V \sim \chi^2_n$ and $U \perp V$ then we say:
$$F = \frac{U/m}{V/n} \sim F_{m,n}$$
or "$F$ has an $F$ distribution with $m$ and $n$ degrees of freedom."

\pause $F$ distributions happen when it makes sense to consider the ratio of sums of squared normals. It is not yet obvious when this might take place. 

\pause The density is nasty, etc.

## pictures of some $F$ distributions

```{r}
z <- seq(0.1, 6, by=0.01)
plot(z, df(z, 1, 30), type="l", col="red", xlab="x", ylab="density")
lines(z, df(z, 2, 25), col="blue")
lines(z, df(z, 10, 20), col="purple")
legend(3, 1, legend = c("df1 = 1, df2=30", "df1 = 2, df2=25", "df1 = 10, df2=20"),
       fill=c("red", "blue", "purple"))
```

